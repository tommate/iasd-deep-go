{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'deep_learning_and_the_game_of_go'...\r\n",
      "remote: Enumerating objects: 33, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (33/33), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (28/28), done.\u001b[K\r\n",
      "remote: Total 894 (delta 9), reused 15 (delta 3), pack-reused 861\u001b[K\r\n",
      "Receiving objects: 100% (894/894), 320.39 MiB | 41.57 MiB/s, done.\r\n",
      "Resolving deltas: 100% (445/445), done.\r\n",
      "Checking out files: 100% (133/133), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone -b chapter_13 https://github.com/maxpumperla/deep_learning_and_the_game_of_go.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 52\r\n",
      "drwxr-xr-x  2 root root 4096 Jan 21 03:18 \u001b[0m\u001b[01;34m.\u001b[0m/\r\n",
      "drwxr-xr-x 16 root root 4096 Jan 21 03:18 \u001b[01;34m..\u001b[0m/\r\n",
      "-rw-r--r--  1 root root  114 Jan 21 03:18 __init__.py\r\n",
      "-rw-r--r--  1 root root 1989 Jan 21 03:18 generator.py\r\n",
      "-rw-r--r--  1 root root 4043 Jan 21 03:18 index_processor.py\r\n",
      "-rw-r--r--  1 root root 8438 Jan 21 03:18 parallel_processor.py\r\n",
      "-rw-r--r--  1 root root 9251 Jan 21 03:18 processor.py\r\n",
      "-rw-r--r--  1 root root 5620 Jan 21 03:18 sampling.py\r\n"
     ]
    }
   ],
   "source": [
    "ls -la deep_learning_and_the_game_of_go/code/dlgo/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20\r\n",
      "drwxr-xr-x 5 root   root    4096 Jan 21 03:18 \u001b[0m\u001b[01;34m.\u001b[0m/\r\n",
      "drwxr-xr-x 6 root   root    4096 Jan 21 03:18 \u001b[01;34m..\u001b[0m/\r\n",
      "drwxr-xr-x 4 nobody nogroup 4096 Jan 21 03:18 \u001b[01;34malphago-dlgo-create-h5py-oneplaneencoder\u001b[0m/\r\n",
      "drwxr-xr-x 4 nobody nogroup 4096 Jan 12 08:28 \u001b[01;34mgo-train-data\u001b[0m/\r\n",
      "drwxr-xr-x 2 nobody nogroup 4096 Jan 13 12:04 \u001b[01;34mgo-train-data-2\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls -la ../input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1021408\r\n",
      "drwxr-xr-x 4 nobody nogroup      4096 Jan 21 03:18 \u001b[0m\u001b[01;34m.\u001b[0m/\r\n",
      "drwxr-xr-x 5 root   root         4096 Jan 21 03:18 \u001b[01;34m..\u001b[0m/\r\n",
      "-rw-r--r-- 1 nobody nogroup   4017526 Jan 21 03:18 __notebook__.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup       589 Jan 21 03:18 __output__.json\r\n",
      "-rw-r--r-- 1 nobody nogroup   3525514 Jan 21 03:18 __results__.html\r\n",
      "-rw-r--r-- 1 nobody nogroup         0 Jan 21 03:18 custom.css\r\n",
      "drwxr-xr-x 2 nobody nogroup      4096 Jan 21 03:18 \u001b[01;34mdata\u001b[0m/\r\n",
      "drwxr-xr-x 5 nobody nogroup      4096 Jan 21 03:18 \u001b[01;34mdeep_learning_and_the_game_of_go\u001b[0m/\r\n",
      "-rw-r--r-- 1 nobody nogroup     42177 Jan 21 03:18 kgs_index.html\r\n",
      "-rw-r--r-- 1 nobody nogroup   1617632 Jan 21 03:18 test_100_oneplane.hdf5\r\n",
      "-rw-r--r-- 1 nobody nogroup      3595 Jan 21 03:18 test_samples.py\r\n",
      "-rw-r--r-- 1 nobody nogroup 155493278 Jan 21 03:18 train_10000_oneplane.hdf5\r\n",
      "-rw-r--r-- 1 nobody nogroup  15172666 Jan 21 03:18 train_1000_oneplane.hdf5\r\n",
      "-rw-r--r-- 1 nobody nogroup   1435644 Jan 21 03:18 train_100_oneplane.hdf5\r\n",
      "-rw-r--r-- 1 nobody nogroup 787069918 Jan 21 03:18 train_50000_oneplane.hdf5\r\n",
      "-rw-r--r-- 1 nobody nogroup  77504463 Jan 21 03:18 train_5000_oneplane.hdf5\r\n"
     ]
    }
   ],
   "source": [
    "ls -la ../input/alphago-dlgo-create-h5py-oneplaneencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('deep_learning_and_the_game_of_go/code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import zipfile, h5py\n",
    "import re, os\n",
    "import numpy as np\n",
    "\n",
    "from dlgo.data.parallel_processor import GoDataProcessor\n",
    "from dlgo.encoders.oneplane import OnePlaneEncoder as PlaneEncoder\n",
    "#from dlgo.encoders.simple import SimpleEncoder as PlaneEncoder\n",
    "from dlgo.networks import small, medium, large\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# from tensorflow.python.keras.models import Sequential\n",
    "# from tensorflow.python.keras.layers.core import Dense\n",
    "# from tensorflow.python.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<_io.BytesIO at 0x7f0fbcadb8e0>, <_io.BytesIO at 0x7f0f788c9fc0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_train_hdf5 = '../input/alphago-dlgo-create-h5py-oneplaneencoder/train_1000_oneplane.hdf5'\n",
    "src_test_hdf5 = '../input/alphago-dlgo-create-h5py-oneplaneencoder/test_100_oneplane.hdf5'\n",
    "with open(src_train_hdf5, 'rb') as f:\n",
    "    hdf5_train_buf = io.BytesIO(f.read())\n",
    "with open(src_test_hdf5, 'rb') as f:\n",
    "    hdf5_test_buf = io.BytesIO(f.read())\n",
    "hdf5_train_buf, hdf5_test_buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<HDF5 file \"<_io.BytesIO object at 0x7f0fbcadb8e0>\" (mode r)>,\n",
       " <HDF5 file \"<_io.BytesIO object at 0x7f0f788c9fc0>\" (mode r)>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf5_train = h5py.File(hdf5_train_buf, \"r\")\n",
    "hdf5_test = h5py.File(hdf5_test_buf, 'r')\n",
    "hdf5_train, hdf5_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 1, 19, 19)\n",
      "(256,)\n",
      "[ 60  73 307 269 268 249 287 231 263 317 174 290  71 327 326  52  51  53\n",
      "  69 111  46 282 262 205 279 315 277 202 299 318 320 319 283 301 245 302\n",
      " 187  97 117  40  59  41  61  58  77  39  78  43  42  23  62  44  63 186\n",
      " 168 147  45  86 106  49  31  84 123  28  26  24  47  48  30 122 104 103\n",
      "  85 159 161 172 173 191 153 197 199  89  70 119 211 192 193 210 141 140\n",
      " 101 120 135 206 207 226 188 225 227 284 129 264 244 143 142 259 278 261\n",
      " 242 260 258 222 280 281 220 241 130 112 246 145 110 181 180 212  72  92\n",
      "  54  35  55  36 235 100 163 223 243 124 162 127 107 108  33  34 184 183\n",
      " 164 165 167  91  90 109  74 131 150 148 149  93 166 185 146  66  65 165\n",
      " 132  56 146 238 239 165 113  75 146 219 200 165 208 203 170  88 105  87\n",
      "  29  60  53 307 269 299 306 325 287 310 324 313 318 317 319 298 263  43\n",
      " 154  91  92  72  73  52 110  54  34  55  33  70  74 111 112 130 131 149\n",
      "  69  89  51  75  93  50  71  90  88 128  49  31 107 127  36  30  48  29\n",
      "  28  47  27  46  45  68  67  86  66  65  85  64  26 104  84 105  87  63\n",
      " 103 122 121 142]\n"
     ]
    }
   ],
   "source": [
    "#print(hdf5_train['data'].keys())\n",
    "print(hdf5_train['data/KGS-2015-19-8133-train_features_0'].shape)\n",
    "print(hdf5_train['data/KGS-2015-19-8133-train_labels_0'].shape)\n",
    "print(np.array(hdf5_train['data/KGS-2015-19-8133-train_labels_0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools, random\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "class Generator(object):\n",
    "    \n",
    "    def __init__(self, hdf5_file, data_type, nn=1):\n",
    "        self.hdf5_file = hdf5_file\n",
    "        self.data_type = data_type\n",
    "        self.nn = nn\n",
    "        self.get_file_list()\n",
    "        self.num_samples = None\n",
    "        self.get_num_samples()\n",
    "    \n",
    "    def get_file_list(self):\n",
    "        name_list0 = list(self.hdf5_file.keys())\n",
    "        #print(name_list0)\n",
    "        name_list = [re.sub('features|labels', '{}', ee) for ee in name_list0]\n",
    "        self.name_list = list(set(name_list))\n",
    "    \n",
    "    def get_num_samples(self, batch_size=128, num_classes=19 * 19):  # <2>\n",
    "        if self.num_samples is not None:\n",
    "            return self.num_samples\n",
    "        else:\n",
    "            self.num_samples = 0\n",
    "            for x, y in self._generate(count_flag=True):\n",
    "                #print(y)\n",
    "                self.num_samples += len(y)\n",
    "            return self.num_samples\n",
    "    \n",
    "#     def _generate(self, batch_size=32, num_classes=19*19, count_flag=False):\n",
    "#         name_list = list(self.name_list)\n",
    "#         random.shuffle(name_list)\n",
    "#         for ifile in itertools.islice(name_list, None):\n",
    "#             file_features = ifile.format('features')\n",
    "#             file_labels = ifile.format('labels')\n",
    "#             #print(file_features, file_labels)\n",
    "#             x = np.array(self.hdf5_file[file_features], dtype='float32')\n",
    "#             x = np.transpose(x, (0,2,3,1))\n",
    "#             y = np.array(self.hdf5_file[file_labels])\n",
    "#             yield x, y\n",
    "    def _generate(self, batch_size=32, num_classes=19*19, count_flag=False):\n",
    "        name_list = list(self.name_list)\n",
    "        if self.data_type == 'test':\n",
    "            nn = 1\n",
    "        else:\n",
    "            nn = self.nn\n",
    "        for _ in range(nn):\n",
    "            random.shuffle(name_list)\n",
    "            for ifile in itertools.islice(name_list, None):\n",
    "                file_features = ifile.format('features')\n",
    "                file_labels = ifile.format('labels')\n",
    "                #print(file_features, file_labels)\n",
    "                x = np.array(self.hdf5_file[file_features], dtype='float32')\n",
    "                x = np.transpose(x, (0,2,3,1))\n",
    "                y = np.array(self.hdf5_file[file_labels])\n",
    "                yield x, y\n",
    "    \n",
    "    def generate(self, batch_size=32, num_classes=19*19):\n",
    "        while True:\n",
    "            for x, y in self._generate(batch_size, num_classes):\n",
    "                #x = x.astype('float32')\n",
    "                y = to_categorical(y.astype(int), num_classes)\n",
    "                #print(x, y)\n",
    "                while x.shape[0] >= batch_size:\n",
    "                    x_batch, x = x[:batch_size], x[batch_size:]\n",
    "                    y_batch, y = y[:batch_size], y[batch_size:]\n",
    "                    yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.84 s, sys: 10.7 ms, total: 1.85 s\n",
      "Wall time: 1.86 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['KGS-2009-19-18837-train_{}_33',\n",
       "  'KGS-2014-19-13029-train_{}_49',\n",
       "  'KGS-2006-19-10388-train_{}_40',\n",
       "  'KGS-2005-19-13941-train_{}_12',\n",
       "  'KGS-2011-19-19099-train_{}_62',\n",
       "  'KGS-2015-19-8133-train_{}_13',\n",
       "  'KGS-2008-19-14002-train_{}_54',\n",
       "  'KGS-2015-19-8133-train_{}_17',\n",
       "  'KGS-2009-19-18837-train_{}_24',\n",
       "  'KGS-2014-19-13029-train_{}_9'],\n",
       " 761,\n",
       " 194816)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "gen_train = Generator(hdf5_train['data'], 'train')\n",
    "#gen_train.name_list\n",
    "#print(gen_train)\n",
    "gen_train.name_list[:10], len(gen_train.name_list), gen_train.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 19, 19, 1) (32, 361)\n",
      "(32, 19, 19, 1) (32, 361)\n",
      "(32, 19, 19, 1) (32, 361)\n"
     ]
    }
   ],
   "source": [
    "for ee in itertools.islice(gen_train.generate(), 3):\n",
    "    x, y = ee\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Generator object at 0x7f0f788285f8>\n",
      "CPU times: user 181 ms, sys: 3.71 ms, total: 185 ms\n",
      "Wall time: 193 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['KGS-2005-19-13941-test_{}_0',\n",
       "  'KGS-2008-19-14002-test_{}_1',\n",
       "  'KGS-2008-19-14002-test_{}_4',\n",
       "  'KGS-2011-19-19099-test_{}_6',\n",
       "  'KGS-2005-19-13941-test_{}_2',\n",
       "  'KGS-2007-19-11644-test_{}_1',\n",
       "  'KGS-2009-19-18837-test_{}_1',\n",
       "  'KGS-2014-19-13029-test_{}_5',\n",
       "  'KGS-2005-19-13941-test_{}_7',\n",
       "  'KGS-2004-19-12106-test_{}_2'],\n",
       " 73,\n",
       " 18688)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "gen_test = Generator(hdf5_test['data'], 'test')\n",
    "print(gen_test)\n",
    "gen_test.name_list[:10], len(gen_test.name_list), gen_test.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 19, 19, 1) (32, 361)\n",
      "(32, 19, 19, 1) (32, 361)\n",
      "(32, 19, 19, 1) (32, 361)\n"
     ]
    }
   ],
   "source": [
    "for ee in itertools.islice(gen_test.generate(), 3):\n",
    "    x, y = ee\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_board_rows, go_board_cols = 19, 19\n",
    "num_classes = go_board_rows * go_board_cols\n",
    "encoder = PlaneEncoder((go_board_rows, go_board_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 7.63 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(194816, 18688)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "gen_train.num_samples, gen_test.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 19, 19, 48)        2400      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 19, 19, 32)        38432     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 19, 19, 32)        25632     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 19, 19, 32)        25632     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 19, 19, 1)         33        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 361)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 361)               130682    \n",
      "=================================================================\n",
      "Total params: 222,811\n",
      "Trainable params: 222,811\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "small\n",
    "'''\n",
    "from keras.layers.core import Dense, Activation, Flatten, Reshape, Lambda, Permute\n",
    "from keras.layers.convolutional import Conv2D, ZeroPadding2D\n",
    "from keras import backend as K\n",
    "# from tensorflow.python.keras.layers.core import Dense, Activation, Flatten, Reshape\n",
    "# from tensorflow.python.keras.layers.convolutional import Conv2D, ZeroPadding2D\n",
    "\n",
    "input_shape = (go_board_rows, go_board_cols, encoder.num_planes)\n",
    "model = Sequential()\n",
    "#model.add(Permute((2,3,1), input_shape=input_shape))\n",
    "model.add(Conv2D(48, (7, 7), padding='same', input_shape=input_shape, activation='elu'))\n",
    "model.add(Conv2D(32, (5, 5), padding='same', activation='elu'))\n",
    "model.add(Conv2D(32, (5, 5), padding='same', activation='elu'))\n",
    "model.add(Conv2D(32, (5, 5), padding='same', activation='elu'))\n",
    "model.add(\n",
    "        Conv2D(filters=1, kernel_size=1, padding='same',\n",
    "               activation='elu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# large\n",
    "# '''\n",
    "# from keras.layers.core import Dense, Activation, Flatten, Reshape\n",
    "# from keras.layers.convolutional import Conv2D, ZeroPadding2D\n",
    "# # from tensorflow.python.keras.layers.core import Dense, Activation, Flatten, Reshape\n",
    "# # from tensorflow.python.keras.layers.convolutional import Conv2D, ZeroPadding2D\n",
    "\n",
    "# input_shape = (encoder.num_planes, go_board_rows, go_board_cols)\n",
    "# #network_layers = small.layers(input_shape)\n",
    "# #network_layers = large.layers(input_shape)\n",
    "# model = Sequential()\n",
    "# # for layer in network_layers:\n",
    "# #     print(layer)\n",
    "# #     model.add(layer)\n",
    "# model.add(Reshape((go_board_rows, go_board_cols, encoder.num_planes), input_shape=input_shape))\n",
    "# model.add(ZeroPadding2D((3, 3)))\n",
    "# model.add(Conv2D(64, (7, 7), padding='valid'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(ZeroPadding2D((2, 2)))\n",
    "# model.add(Conv2D(64, (5, 5)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(ZeroPadding2D((2, 2)))\n",
    "# model.add(Conv2D(64, (5, 5)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(ZeroPadding2D((2, 2)))\n",
    "# model.add(Conv2D(48, (5, 5)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(ZeroPadding2D((2, 2)))\n",
    "# model.add(Conv2D(48, (5, 5)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(ZeroPadding2D((2, 2)))\n",
    "# model.add(Conv2D(32, (5, 5)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(ZeroPadding2D((2, 2)))\n",
    "# model.add(Conv2D(32, (5, 5)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(1024))\n",
    "# model.add(Activation('relu'))\n",
    "        \n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "# #model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<tf.Tensor 'conv2d_1_input:0' shape=(None, 19, 19, 1) dtype=float32>],\n",
       " [<tf.Tensor 'dense_1/Softmax:0' shape=(None, 361) dtype=float32>])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inputs, model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      " - 46s - loss: 3.5722 - accuracy: 0.3832 - val_loss: 5.1519 - val_accuracy: 0.3162\n",
      "Epoch 2/4\n",
      " - 41s - loss: 3.2714 - accuracy: 0.3932 - val_loss: 4.9412 - val_accuracy: 0.3221\n",
      "Epoch 3/4\n",
      " - 42s - loss: 3.1258 - accuracy: 0.4021 - val_loss: 5.1080 - val_accuracy: 0.3283\n",
      "Epoch 4/4\n",
      " - 41s - loss: 3.0064 - accuracy: 0.4121 - val_loss: 5.1068 - val_accuracy: 0.3352\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "batch_size = 32\n",
    "hst = model.fit_generator(generator=gen_train.generate(batch_size, num_classes),\n",
    "                    epochs=epochs, verbose=2,\n",
    "                    steps_per_epoch=gen_train.num_samples / batch_size,\n",
    "                    validation_data=gen_test.generate(batch_size, num_classes),\n",
    "                    validation_steps=gen_test.num_samples / batch_size,\n",
    "                    callbacks=[ModelCheckpoint('checkpoints/model_epoch_{epoch}.h5')])\n",
    "# hst = model.fit_generator(generator=gen_train.generate(batch_size, num_classes),\n",
    "#                     epochs=epochs,\n",
    "#                     steps_per_epoch=5,\n",
    "#                     validation_data=gen_test.generate(batch_size, num_classes),\n",
    "#                     validation_steps=5,\n",
    "#                     callbacks=[ModelCheckpoint('checkpoints/model_epoch_{epoch}.h5')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      " - 24s - loss: 2.8386 - accuracy: 0.4287 - val_loss: 3.9640 - val_accuracy: 0.3444\n",
      "Epoch 2/4\n",
      " - 24s - loss: 2.7012 - accuracy: 0.4455 - val_loss: 5.5618 - val_accuracy: 0.3555\n",
      "Epoch 3/4\n",
      " - 25s - loss: 2.5730 - accuracy: 0.4654 - val_loss: 1.5591e-04 - val_accuracy: 0.3665\n",
      "Epoch 4/4\n",
      " - 24s - loss: 2.4459 - accuracy: 0.4868 - val_loss: 7.9390e-05 - val_accuracy: 0.3769\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "batch_size = 64\n",
    "hst = model.fit_generator(generator=gen_train.generate(batch_size, num_classes),\n",
    "                    epochs=epochs, verbose=2,\n",
    "                    steps_per_epoch=gen_train.num_samples / batch_size,\n",
    "                    validation_data=gen_test.generate(batch_size, num_classes),\n",
    "                    validation_steps=gen_test.num_samples / batch_size,\n",
    "                    callbacks=[ModelCheckpoint('checkpoints/model_epoch_{epoch}.h5')])\n",
    "# hst = model.fit_generator(generator=gen_train.generate(batch_size, num_classes),\n",
    "#                     epochs=epochs,\n",
    "#                     steps_per_epoch=5,\n",
    "#                     validation_data=gen_test.generate(batch_size, num_classes),\n",
    "#                     validation_steps=5,\n",
    "#                     callbacks=[ModelCheckpoint('checkpoints/model_epoch_{epoch}.h5')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      " - 15s - loss: 2.2929 - accuracy: 0.5103 - val_loss: 1.8154e-04 - val_accuracy: 0.3875\n",
      "Epoch 2/4\n",
      " - 16s - loss: 2.1932 - accuracy: 0.5258 - val_loss: 5.1389 - val_accuracy: 0.3912\n",
      "Epoch 3/4\n",
      " - 15s - loss: 2.1045 - accuracy: 0.5403 - val_loss: 4.4787 - val_accuracy: 0.3973\n",
      "Epoch 4/4\n",
      " - 15s - loss: 2.0298 - accuracy: 0.5518 - val_loss: 4.6118 - val_accuracy: 0.4005\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "batch_size = 128\n",
    "hst = model.fit_generator(generator=gen_train.generate(batch_size, num_classes),\n",
    "                    epochs=epochs, verbose=2,\n",
    "                    steps_per_epoch=gen_train.num_samples / batch_size,\n",
    "                    validation_data=gen_test.generate(batch_size, num_classes),\n",
    "                    validation_steps=gen_test.num_samples / batch_size,\n",
    "                    callbacks=[ModelCheckpoint('checkpoints/model_epoch_{epoch}.h5')])\n",
    "# hst = model.fit_generator(generator=gen_train.generate(batch_size, num_classes),\n",
    "#                     epochs=epochs,\n",
    "#                     steps_per_epoch=5,\n",
    "#                     validation_data=gen_test.generate(batch_size, num_classes),\n",
    "#                     validation_steps=5,\n",
    "#                     callbacks=[ModelCheckpoint('checkpoints/model_epoch_{epoch}.h5')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.optimizers import Adam\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 5\n",
    "# batch_size = 32\n",
    "# hst = model.fit_generator(generator=gen_train.generate(batch_size, num_classes),\n",
    "#                     epochs=epochs,\n",
    "#                     steps_per_epoch=gen_train.num_samples / batch_size,\n",
    "#                     validation_data=gen_test.generate(batch_size, num_classes),\n",
    "#                     validation_steps=gen_test.num_samples / batch_size,\n",
    "#                     callbacks=[ModelCheckpoint('checkpoints/model_epoch_{epoch}.h5')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 10632\r\n",
      "drwxr-xr-x 2 root root    4096 Jan 21 03:21 \u001b[0m\u001b[01;34m.\u001b[0m/\r\n",
      "drwxr-xr-x 4 root root    4096 Jan 21 03:18 \u001b[01;34m..\u001b[0m/\r\n",
      "-rw-r--r-- 1 root root 2716876 Jan 21 03:23 model_epoch_1.h5\r\n",
      "-rw-r--r-- 1 root root 2716876 Jan 21 03:23 model_epoch_2.h5\r\n",
      "-rw-r--r-- 1 root root 2716876 Jan 21 03:24 model_epoch_3.h5\r\n",
      "-rw-r--r-- 1 root root 2716876 Jan 21 03:24 model_epoch_4.h5\r\n"
     ]
    }
   ],
   "source": [
    "ls -la checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
