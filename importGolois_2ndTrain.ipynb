{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldvZjK8YrTVG"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNxDy5hTfHs_"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/cours/iasd-deep-golois/project/\n",
    "!pip install pybind11\n",
    "!chmod 777 ./compile.sh\n",
    "!./compile.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVnZETentMAk"
   },
   "outputs": [],
   "source": [
    "!cp golois.cpython-36m-x86_64-linux-gnu.so golois.so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MCfCVVRP6LN"
   },
   "source": [
    "# Legrand & Rausell, 2Ã©me tournoi\n",
    "\n",
    "## Best solution inspired from \"Mobile Networks for Computer Go\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mwH82ruCTEsF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/project\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "(10000, 19, 19, 21)\n",
      "getValidation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, DepthwiseConv2D, Dense, Concatenate, Add, ReLU, BatchNormalization, AvgPool2D, MaxPool2D, GlobalAveragePooling2D, Reshape, Permute, Lambda, Flatten, Activation\n",
    "\n",
    "import golois\n",
    "\n",
    "planes = 21\n",
    "moves = 361\n",
    "N = 10000\n",
    "epochs = 500\n",
    "# epochs = 20\n",
    "batch=64 \n",
    "filters = 64\n",
    "trunk = filters\n",
    "\n",
    "input_data = np.random.randint(2, size=(N, 19, 19, planes))\n",
    "input_data = input_data.astype ('float32')\n",
    "\n",
    "print(input_data.ndim)\n",
    "print(input_data.shape)\n",
    "\n",
    "policy = np.random.randint(moves, size=(N,))\n",
    "policy = keras.utils.to_categorical (policy)\n",
    "\n",
    "value = np.random.randint(2, size=(N,))\n",
    "value = value.astype ('float32')\n",
    "\n",
    "end = np.random.randint(2, size=(N, 19, 19, 2))\n",
    "end = end.astype ('float32')\n",
    "\n",
    "groups = np.zeros((N, 19, 19, 1))\n",
    "groups = groups.astype ('float32')\n",
    "\n",
    "print (\"getValidation\", flush = True)\n",
    "golois.getValidation (input_data, policy, value, end)\n",
    "\n",
    "def bottleneck_block_mobilenet(x, expand=512, squeeze=128):\n",
    "# def bottleneck_block_mobilenet(x, expand=96, squeeze=16):\n",
    "    m = layers.Conv2D(expand, (1,1), kernel_regularizer=regularizers.l2(0.0001), use_bias = False)(x)\n",
    "    m = layers.BatchNormalization()(m)\n",
    "    m = layers.Activation('relu')(m)\n",
    "    m = layers.DepthwiseConv2D((3,3), padding='same', kernel_regularizer=regularizers.l2(0.0001), use_bias = False)(m)\n",
    "    m = layers.BatchNormalization()(m)\n",
    "    m = layers.Activation('relu')(m)\n",
    "    m = layers.Conv2D(squeeze, (1,1), kernel_regularizer=regularizers.l2(0.0001), use_bias = False)(m)\n",
    "    m = layers.BatchNormalization()(m)\n",
    "    return layers.Add()([m, x])\n",
    "\n",
    "def bottleneck_block_shufflenet(tensor, expand=512, squeeze=128):\n",
    "# def bottleneck_block_shufflenet(tensor, expand, squeeze=64):\n",
    "    x = gconv(tensor, channels=expand, groups=4)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = channel_shuffle(x, groups=4)\n",
    "    x = DepthwiseConv2D(kernel_size=3, padding='same', kernel_regularizer=regularizers.l2(0.0001), use_bias = False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = gconv(x, channels=squeeze, groups=4)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Add()([tensor, x])\n",
    "    output = ReLU()(x)\n",
    "    return output\n",
    "\n",
    "def gconv(tensor, channels, groups):\n",
    "    input_ch = tensor.get_shape().as_list()[-1]\n",
    "    group_ch = input_ch // groups\n",
    "    output_ch = channels // groups\n",
    "    groups_list = []\n",
    "\n",
    "    for i in range(groups):\n",
    "        group_tensor = tensor[:, :, :, i * group_ch: (i+1) * group_ch]\n",
    "        group_tensor = Conv2D(output_ch, 1)(group_tensor)\n",
    "        groups_list.append(group_tensor)\n",
    "\n",
    "    output = Concatenate()(groups_list)\n",
    "    return output\n",
    "\n",
    "def gconv(tensor, channels, groups):\n",
    "    input_ch = tensor.get_shape().as_list()[-1]\n",
    "    group_ch = input_ch // groups\n",
    "    output_ch = channels // groups\n",
    "    groups_list = []\n",
    "\n",
    "    for i in range(groups):\n",
    "        group_tensor = tensor[:, :, :, i * group_ch: (i+1) * group_ch]\n",
    "        group_tensor = Conv2D(output_ch, 1)(group_tensor)\n",
    "        groups_list.append(group_tensor)\n",
    "\n",
    "    output = Concatenate()(groups_list)\n",
    "    return output\n",
    "\n",
    "def channel_shuffle(x, groups):  \n",
    "    _, width, height, channels = x.get_shape().as_list()\n",
    "    group_ch = channels // groups\n",
    "\n",
    "    x = Reshape([width, height, group_ch, groups])(x)\n",
    "    x = Permute([1, 2, 4, 3])(x)\n",
    "    x = Reshape([width, height, channels])(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "board (InputLayer)              [(None, 19, 19, 21)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 19, 19, 64)   1408        board[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 19, 19, 64)   256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 19, 19, 64)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 19, 19, 198)  12672       re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 19, 19, 198)  792         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 19, 19, 198)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d (DepthwiseConv (None, 19, 19, 198)  1782        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 19, 19, 198)  792         depthwise_conv2d[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 19, 19, 198)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 19, 19, 64)   12672       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 19, 19, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 19, 19, 64)   0           batch_normalization_3[0][0]      \n",
      "                                                                 re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 19, 19, 198)  12672       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 19, 19, 198)  792         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 19, 19, 198)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_1 (DepthwiseCo (None, 19, 19, 198)  1782        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 19, 19, 198)  792         depthwise_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 19, 19, 198)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 19, 19, 64)   12672       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 19, 19, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 19, 19, 64)   0           batch_normalization_6[0][0]      \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 19, 19, 198)  12672       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 19, 19, 198)  792         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 19, 19, 198)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_2 (DepthwiseCo (None, 19, 19, 198)  1782        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 19, 19, 198)  792         depthwise_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 19, 19, 198)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 19, 19, 64)   12672       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 19, 19, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 19, 19, 64)   0           batch_normalization_9[0][0]      \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 19, 19, 198)  12672       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 19, 19, 198)  792         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 19, 19, 198)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_3 (DepthwiseCo (None, 19, 19, 198)  1782        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 19, 19, 198)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 19, 19, 64)   12672       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 19, 19, 64)   256         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 19, 19, 64)   0           batch_normalization_12[0][0]     \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 19, 19, 198)  12672       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 19, 19, 198)  792         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 19, 19, 198)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_4 (DepthwiseCo (None, 19, 19, 198)  1782        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 19, 19, 198)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 19, 19, 64)   12672       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 19, 19, 64)   256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 19, 19, 64)   0           batch_normalization_15[0][0]     \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 19, 19, 198)  12672       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 19, 19, 198)  792         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 19, 19, 198)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_5 (DepthwiseCo (None, 19, 19, 198)  1782        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 19, 19, 198)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 19, 19, 64)   12672       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 19, 19, 64)   256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 19, 19, 64)   0           batch_normalization_18[0][0]     \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 19, 19, 198)  12672       add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 19, 19, 198)  792         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 19, 19, 198)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_6 (DepthwiseCo (None, 19, 19, 198)  1782        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 19, 19, 198)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 19, 19, 64)   12672       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 19, 19, 64)   256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 19, 19, 64)   0           batch_normalization_21[0][0]     \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 19, 19, 198)  12672       add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 19, 19, 198)  792         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 19, 19, 198)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_7 (DepthwiseCo (None, 19, 19, 198)  1782        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 19, 19, 198)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 19, 19, 64)   12672       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 19, 19, 64)   256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 19, 19, 64)   0           batch_normalization_24[0][0]     \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 19, 19, 198)  12672       add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 19, 19, 198)  792         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 19, 19, 198)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_8 (DepthwiseCo (None, 19, 19, 198)  1782        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 19, 19, 198)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 19, 19, 64)   12672       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 19, 19, 64)   256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 19, 19, 64)   0           batch_normalization_27[0][0]     \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 19, 19, 198)  12672       add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 19, 19, 198)  792         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 19, 19, 198)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_9 (DepthwiseCo (None, 19, 19, 198)  1782        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 19, 19, 198)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 19, 19, 64)   12672       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 19, 19, 64)   256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 19, 19, 64)   0           batch_normalization_30[0][0]     \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 19, 19, 198)  12672       add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 19, 19, 198)  792         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 19, 19, 198)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_10 (DepthwiseC (None, 19, 19, 198)  1782        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 19, 19, 198)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 19, 19, 64)   12672       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 19, 19, 64)   256         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 19, 19, 64)   0           batch_normalization_33[0][0]     \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 19, 19, 198)  12672       add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 19, 19, 198)  792         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 19, 19, 198)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_11 (DepthwiseC (None, 19, 19, 198)  1782        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 19, 19, 198)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 19, 19, 64)   12672       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 19, 19, 64)   256         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 19, 19, 64)   0           batch_normalization_36[0][0]     \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 19, 19, 198)  12672       add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 19, 19, 198)  792         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 19, 19, 198)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_12 (DepthwiseC (None, 19, 19, 198)  1782        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 19, 19, 198)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 19, 19, 64)   12672       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 19, 19, 64)   256         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 19, 19, 64)   0           batch_normalization_39[0][0]     \n",
      "                                                                 add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 19, 19, 198)  12672       add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 19, 19, 198)  792         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 19, 19, 198)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_13 (DepthwiseC (None, 19, 19, 198)  1782        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 19, 19, 198)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 19, 19, 64)   12672       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 19, 19, 64)   256         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 19, 19, 64)   0           batch_normalization_42[0][0]     \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 19, 19, 198)  12672       add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 19, 19, 198)  792         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 19, 19, 198)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_14 (DepthwiseC (None, 19, 19, 198)  1782        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 19, 19, 198)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 19, 19, 64)   12672       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 19, 19, 64)   256         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 19, 19, 64)   0           batch_normalization_45[0][0]     \n",
      "                                                                 add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 19, 19, 198)  12672       add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 19, 19, 198)  792         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 19, 19, 198)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_15 (DepthwiseC (None, 19, 19, 198)  1782        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 19, 19, 198)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 19, 19, 64)   12672       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 19, 19, 64)   256         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 19, 19, 64)   0           batch_normalization_48[0][0]     \n",
      "                                                                 add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 19, 19, 198)  12672       add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 19, 19, 198)  792         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 19, 19, 198)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_16 (DepthwiseC (None, 19, 19, 198)  1782        activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 19, 19, 198)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 19, 19, 64)   12672       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 19, 19, 64)   256         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 19, 19, 64)   0           batch_normalization_51[0][0]     \n",
      "                                                                 add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 19, 19, 198)  12672       add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 19, 19, 198)  792         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 19, 19, 198)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_17 (DepthwiseC (None, 19, 19, 198)  1782        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 19, 19, 198)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 19, 19, 64)   12672       activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 19, 19, 64)   256         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 19, 19, 64)   0           batch_normalization_54[0][0]     \n",
      "                                                                 add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 19, 19, 198)  12672       add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 19, 19, 198)  792         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 19, 19, 198)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_18 (DepthwiseC (None, 19, 19, 198)  1782        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 19, 19, 198)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 19, 19, 64)   12672       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 19, 19, 64)   256         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 19, 19, 64)   0           batch_normalization_57[0][0]     \n",
      "                                                                 add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 19, 19, 198)  12672       add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 19, 19, 198)  792         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 19, 19, 198)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_19 (DepthwiseC (None, 19, 19, 198)  1782        activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 19, 19, 198)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 19, 19, 64)   12672       activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 19, 19, 64)   256         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 19, 19, 64)   0           batch_normalization_60[0][0]     \n",
      "                                                                 add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 19, 19, 198)  12672       add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 19, 19, 198)  792         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 19, 19, 198)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_20 (DepthwiseC (None, 19, 19, 198)  1782        activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_20[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 19, 19, 198)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 19, 19, 64)   12672       activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 19, 19, 64)   256         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 19, 19, 64)   0           batch_normalization_63[0][0]     \n",
      "                                                                 add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 19, 19, 198)  12672       add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 19, 19, 198)  792         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 19, 19, 198)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_21 (DepthwiseC (None, 19, 19, 198)  1782        activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_21[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 19, 19, 198)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 19, 19, 64)   12672       activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 19, 19, 64)   256         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 19, 19, 64)   0           batch_normalization_66[0][0]     \n",
      "                                                                 add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 19, 19, 198)  12672       add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 19, 19, 198)  792         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 19, 19, 198)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_22 (DepthwiseC (None, 19, 19, 198)  1782        activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 19, 19, 198)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 19, 19, 64)   12672       activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 19, 19, 64)   256         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 19, 19, 64)   0           batch_normalization_69[0][0]     \n",
      "                                                                 add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 19, 19, 198)  12672       add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 19, 19, 198)  792         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 19, 19, 198)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_23 (DepthwiseC (None, 19, 19, 198)  1782        activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_23[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 19, 19, 198)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 19, 19, 64)   12672       activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 19, 19, 64)   256         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 19, 19, 64)   0           batch_normalization_72[0][0]     \n",
      "                                                                 add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 19, 19, 198)  12672       add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 19, 19, 198)  792         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 19, 19, 198)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_24 (DepthwiseC (None, 19, 19, 198)  1782        activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_24[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 19, 19, 198)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 19, 19, 64)   12672       activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 19, 19, 64)   256         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 19, 19, 64)   0           batch_normalization_75[0][0]     \n",
      "                                                                 add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 19, 19, 198)  12672       add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 19, 19, 198)  792         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 19, 19, 198)  0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_25 (DepthwiseC (None, 19, 19, 198)  1782        activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_25[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 19, 19, 198)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 19, 19, 64)   12672       activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 19, 19, 64)   256         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 19, 19, 64)   0           batch_normalization_78[0][0]     \n",
      "                                                                 add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 19, 19, 198)  12672       add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 19, 19, 198)  792         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 19, 19, 198)  0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_26 (DepthwiseC (None, 19, 19, 198)  1782        activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_26[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 19, 19, 198)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 19, 19, 64)   12672       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 19, 19, 64)   256         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 19, 19, 64)   0           batch_normalization_81[0][0]     \n",
      "                                                                 add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 19, 19, 198)  12672       add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 19, 19, 198)  792         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 19, 19, 198)  0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_27 (DepthwiseC (None, 19, 19, 198)  1782        activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_27[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 19, 19, 198)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 19, 19, 64)   12672       activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 19, 19, 64)   256         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 19, 19, 64)   0           batch_normalization_84[0][0]     \n",
      "                                                                 add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 19, 19, 198)  12672       add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 19, 19, 198)  792         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 19, 19, 198)  0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_28 (DepthwiseC (None, 19, 19, 198)  1782        activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_28[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 19, 19, 198)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 19, 19, 64)   12672       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 19, 19, 64)   256         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 19, 19, 64)   0           batch_normalization_87[0][0]     \n",
      "                                                                 add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 19, 19, 198)  12672       add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 19, 19, 198)  792         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 19, 19, 198)  0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_29 (DepthwiseC (None, 19, 19, 198)  1782        activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_29[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 19, 19, 198)  0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 19, 19, 64)   12672       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 19, 19, 64)   256         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 19, 19, 64)   0           batch_normalization_90[0][0]     \n",
      "                                                                 add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 19, 19, 198)  12672       add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 19, 19, 198)  792         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 19, 19, 198)  0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_30 (DepthwiseC (None, 19, 19, 198)  1782        activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_30[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 19, 19, 198)  0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 19, 19, 64)   12672       activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 19, 19, 64)   256         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 19, 19, 64)   0           batch_normalization_93[0][0]     \n",
      "                                                                 add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 19, 19, 198)  12672       add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 19, 19, 198)  792         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 19, 19, 198)  0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_31 (DepthwiseC (None, 19, 19, 198)  1782        activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_31[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 19, 19, 198)  0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 19, 19, 64)   12672       activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 19, 19, 64)   256         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 19, 19, 64)   0           batch_normalization_96[0][0]     \n",
      "                                                                 add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 19, 19, 198)  12672       add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 19, 19, 198)  792         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 19, 19, 198)  0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_32 (DepthwiseC (None, 19, 19, 198)  1782        activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 19, 19, 198)  792         depthwise_conv2d_32[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 19, 19, 198)  0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 19, 19, 64)   12672       activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 19, 19, 64)   256         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 19, 19, 64)   0           batch_normalization_99[0][0]     \n",
      "                                                                 add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 19, 19, 1)    64          add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 64)           0           add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 361)          0           conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 50)           3250        global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "policy (Activation)             (None, 361)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "value (Dense)                   (None, 1)            51          dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 960,907\n",
      "Trainable params: 930,419\n",
      "Non-trainable params: 30,488\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def getModel ():\n",
    "#    input = keras.Input(shape=(19, 19, planes), name='board')\n",
    "#    x = Conv2D(trunk, 1, padding='same', kernel_regularizer=regularizers.l2(0.0001))(input)\n",
    "#    x = BatchNormalization()(x)\n",
    "#    x = ReLU()(x)\n",
    "#    for i in range (blocks):\n",
    "#        x = bottleneck_block_shufflenet (x, filters, trunk)\n",
    "#    policy_head = Conv2D(1, 1, activation='relu', padding='same', use_bias = False, kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "#    policy_head = Flatten()(policy_head)\n",
    "#    policy_head = Activation('softmax', name='policy')(policy_head)\n",
    "#    value_head = GlobalAveragePooling2D()(x)\n",
    "#    value_head = Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(value_head)\n",
    "#    value_head = Dense(1, activation='sigmoid', name='value', kernel_regularizer=regularizers.l2(0.0001))(value_head)\n",
    "        \n",
    "#    model = keras.Model(inputs=input, outputs=[policy_head, value_head])\n",
    "#    return model\n",
    "\n",
    "\n",
    "input = keras.Input(shape=(19, 19, planes), name='board')\n",
    "\n",
    "# This one is the mobile net (Thomas' version)\n",
    "# x = layers.Conv2D(trunk, 1, activation='relu', padding='same')(input)\n",
    "# for i in range (blocks):\n",
    "#    x = bottleneck_block_mobilenet(x, filters, trunk)\n",
    "# This one is the mobile net (Antonio's version)\n",
    "x = layers.Conv2D(trunk, 1, padding='same',kernel_regularizer=regularizers.l2(0.0001))(input)\n",
    "x_bifurcation = BatchNormalization()(x)\n",
    "\n",
    "blocks = 33 # 33\n",
    "filters = 198 # 200\n",
    "x = ReLU()(x_bifurcation)\n",
    "for i in range (blocks):\n",
    "    x = bottleneck_block_mobilenet(x, filters, trunk)\n",
    "\n",
    "# In series\n",
    "# for i in range (blocks):\n",
    "#    x = bottleneck_block_shufflenet(x, filters, trunk)\n",
    "\n",
    "\n",
    "# In parallel\n",
    "# y = ReLU()(x_bifurcation)\n",
    "# for i in range (blocks):\n",
    "#    y = bottleneck_block_shufflenet(y, filters, trunk)\n",
    "# concatenate\n",
    "# x = layers.concatenate([x, y], axis=-1)\n",
    "\n",
    "    \n",
    "# fully convolutional, no dense layer\n",
    "policy_head = layers.Conv2D(1, 1, activation='relu', padding='same', use_bias = False, kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "policy_head = layers.Flatten()(policy_head)\n",
    "policy_head = layers.Activation('softmax', name='policy')(policy_head)\n",
    "\n",
    "value_head = layers.GlobalAveragePooling2D()(x)\n",
    "value_head = layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.0001))(value_head)\n",
    "value_head = layers.Dense(1, activation='sigmoid', name='value',kernel_regularizer=regularizers.l2(0.0001))(value_head)\n",
    "\n",
    "model = keras.Model(inputs=input, outputs=[policy_head, value_head])\n",
    "\n",
    "model.summary ()\n",
    "\n",
    "# 2nd Train momentum 0.8; 2nd Train momentum 0.9\n",
    "# model.compile(optimizer=keras.optimizers.SGD(lr=0.000001, momentum=0.9,nesterov=False),\n",
    "# 2nd Train\n",
    "model.compile(optimizer=keras.optimizers.SGD(lr=0.005, momentum=0.9),\n",
    "              loss={'policy': 'categorical_crossentropy', 'value': 'binary_crossentropy'},\n",
    "              # 2nd Train\n",
    "              loss_weights={'policy' : 2.0, 'value' : 1.0},\n",
    "              # loss_weights={'policy' : 1.0, 'value' : 1.0},\n",
    "              metrics={'policy': 'categorical_accuracy', 'value': 'mse'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJVlA8LLYWjg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "157/157 [==============================] - 27s 105ms/step - loss: 4.4571 - policy_loss: 1.8323 - value_loss: 0.5680 - policy_categorical_accuracy: 0.4795 - value_mse: 0.1981\n",
      "\n",
      "Epoch 00001: loss improved from inf to 4.47047, saving model to checkpoint/cp.ckpt\n",
      "epoch 2\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 4.4118 - policy_loss: 1.8109 - value_loss: 0.5655 - policy_categorical_accuracy: 0.4811 - value_mse: 0.1985\n",
      "\n",
      "Epoch 00001: loss improved from 4.47047 to 4.41176, saving model to checkpoint/cp.ckpt\n",
      "epoch 3\n",
      "157/157 [==============================] - 18s 114ms/step - loss: 4.4186 - policy_loss: 1.8172 - value_loss: 0.5597 - policy_categorical_accuracy: 0.4801 - value_mse: 0.1960\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.41176\n",
      "epoch 4\n",
      "157/157 [==============================] - 17s 107ms/step - loss: 4.4734 - policy_loss: 1.8456 - value_loss: 0.5577 - policy_categorical_accuracy: 0.4740 - value_mse: 0.1950\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.41176\n",
      "epoch 5\n",
      "157/157 [==============================] - 17s 107ms/step - loss: 4.4073 - policy_loss: 1.8088 - value_loss: 0.5652 - policy_categorical_accuracy: 0.4810 - value_mse: 0.1980\n",
      "\n",
      "Epoch 00001: loss improved from 4.41176 to 4.40731, saving model to checkpoint/cp.ckpt\n",
      "epoch 6\n",
      "157/157 [==============================] - 17s 107ms/step - loss: 4.4025 - policy_loss: 1.8094 - value_loss: 0.5591 - policy_categorical_accuracy: 0.4900 - value_mse: 0.1958\n",
      "\n",
      "Epoch 00001: loss improved from 4.40731 to 4.40245, saving model to checkpoint/cp.ckpt\n",
      "epoch 7\n",
      "157/157 [==============================] - 17s 107ms/step - loss: 4.4203 - policy_loss: 1.8158 - value_loss: 0.5641 - policy_categorical_accuracy: 0.4777 - value_mse: 0.1978\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.40245\n",
      "epoch 8\n",
      "157/157 [==============================] - 16s 105ms/step - loss: 4.3982 - policy_loss: 1.8049 - value_loss: 0.5638 - policy_categorical_accuracy: 0.4811 - value_mse: 0.1971\n",
      "\n",
      "Epoch 00001: loss improved from 4.40245 to 4.39820, saving model to checkpoint/cp.ckpt\n",
      "epoch 9\n",
      "157/157 [==============================] - 17s 106ms/step - loss: 4.3819 - policy_loss: 1.8042 - value_loss: 0.5490 - policy_categorical_accuracy: 0.4872 - value_mse: 0.1920\n",
      "\n",
      "Epoch 00001: loss improved from 4.39820 to 4.38190, saving model to checkpoint/cp.ckpt\n",
      "epoch 10\n",
      "157/157 [==============================] - 17s 106ms/step - loss: 4.4011 - policy_loss: 1.8136 - value_loss: 0.5494 - policy_categorical_accuracy: 0.4829 - value_mse: 0.1918\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.38190\n",
      "val = [4.356232643127441, 1.7922484874725342, 0.5471835732460022, 0.48559999465942383, 0.19131527841091156]\n",
      "epoch 11\n",
      "157/157 [==============================] - 17s 106ms/step - loss: 4.3694 - policy_loss: 1.7954 - value_loss: 0.5541 - policy_categorical_accuracy: 0.4860 - value_mse: 0.1933\n",
      "\n",
      "Epoch 00001: loss improved from 4.38190 to 4.36943, saving model to checkpoint/cp.ckpt\n",
      "epoch 12\n",
      "157/157 [==============================] - 16s 103ms/step - loss: 4.4190 - policy_loss: 1.8200 - value_loss: 0.5545 - policy_categorical_accuracy: 0.4825 - value_mse: 0.1939\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.36943\n",
      "epoch 13\n",
      "157/157 [==============================] - 16s 104ms/step - loss: 4.4371 - policy_loss: 1.8295 - value_loss: 0.5536 - policy_categorical_accuracy: 0.4735 - value_mse: 0.1934\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.36943\n",
      "epoch 14\n",
      "157/157 [==============================] - 16s 103ms/step - loss: 4.4300 - policy_loss: 1.8302 - value_loss: 0.5450 - policy_categorical_accuracy: 0.4791 - value_mse: 0.1906\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.36943\n",
      "epoch 15\n",
      "157/157 [==============================] - 17s 106ms/step - loss: 4.3757 - policy_loss: 1.7984 - value_loss: 0.5544 - policy_categorical_accuracy: 0.4785 - value_mse: 0.1937\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.36943\n",
      "epoch 16\n",
      "157/157 [==============================] - 16s 105ms/step - loss: 4.4535 - policy_loss: 1.8391 - value_loss: 0.5507 - policy_categorical_accuracy: 0.4787 - value_mse: 0.1919\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.36943\n",
      "epoch 17\n",
      "157/157 [==============================] - 17s 106ms/step - loss: 4.3152 - policy_loss: 1.7741 - value_loss: 0.5424 - policy_categorical_accuracy: 0.4901 - value_mse: 0.1892\n",
      "\n",
      "Epoch 00001: loss improved from 4.36943 to 4.31523, saving model to checkpoint/cp.ckpt\n",
      "epoch 18\n",
      "157/157 [==============================] - 16s 103ms/step - loss: 4.4106 - policy_loss: 1.8207 - value_loss: 0.5447 - policy_categorical_accuracy: 0.4788 - value_mse: 0.1898\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 19\n",
      "157/157 [==============================] - 17s 106ms/step - loss: 4.3889 - policy_loss: 1.8072 - value_loss: 0.5499 - policy_categorical_accuracy: 0.4857 - value_mse: 0.1920\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 20\n",
      "157/157 [==============================] - 17s 107ms/step - loss: 4.4369 - policy_loss: 1.8340 - value_loss: 0.5443 - policy_categorical_accuracy: 0.4749 - value_mse: 0.1898\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "val = [4.3387908935546875, 1.7871794700622559, 0.5398806929588318, 0.4869000017642975, 0.18840020895004272]\n",
      "epoch 21\n",
      "157/157 [==============================] - 17s 107ms/step - loss: 4.4535 - policy_loss: 1.8394 - value_loss: 0.5502 - policy_categorical_accuracy: 0.4739 - value_mse: 0.1915\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 22\n",
      "157/157 [==============================] - 17s 106ms/step - loss: 4.4317 - policy_loss: 1.8292 - value_loss: 0.5487 - policy_categorical_accuracy: 0.4830 - value_mse: 0.1916\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 23\n",
      "157/157 [==============================] - 16s 104ms/step - loss: 4.3858 - policy_loss: 1.8128 - value_loss: 0.5356 - policy_categorical_accuracy: 0.4802 - value_mse: 0.1866\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 24\n",
      "157/157 [==============================] - 16s 105ms/step - loss: 4.4053 - policy_loss: 1.8188 - value_loss: 0.5430 - policy_categorical_accuracy: 0.4764 - value_mse: 0.1893\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 25\n",
      "157/157 [==============================] - 16s 105ms/step - loss: 4.3467 - policy_loss: 1.7915 - value_loss: 0.5391 - policy_categorical_accuracy: 0.4821 - value_mse: 0.1876\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 26\n",
      "157/157 [==============================] - 16s 104ms/step - loss: 4.3913 - policy_loss: 1.8129 - value_loss: 0.5409 - policy_categorical_accuracy: 0.4807 - value_mse: 0.1883\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 27\n",
      "157/157 [==============================] - 17s 105ms/step - loss: 4.3680 - policy_loss: 1.8024 - value_loss: 0.5387 - policy_categorical_accuracy: 0.4816 - value_mse: 0.1874\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 28\n",
      "157/157 [==============================] - 16s 103ms/step - loss: 4.3665 - policy_loss: 1.8000 - value_loss: 0.5419 - policy_categorical_accuracy: 0.4835 - value_mse: 0.1890\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 29\n",
      "157/157 [==============================] - 16s 103ms/step - loss: 4.4363 - policy_loss: 1.8347 - value_loss: 0.5423 - policy_categorical_accuracy: 0.4748 - value_mse: 0.1892\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 30\n",
      "157/157 [==============================] - 17s 105ms/step - loss: 4.4453 - policy_loss: 1.8375 - value_loss: 0.5458 - policy_categorical_accuracy: 0.4810 - value_mse: 0.1903\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "val = [4.327659606933594, 1.783646821975708, 0.5358170866966248, 0.4869000017642975, 0.1867540180683136]\n",
      "epoch 31\n",
      "157/157 [==============================] - 17s 105ms/step - loss: 4.4230 - policy_loss: 1.8282 - value_loss: 0.5421 - policy_categorical_accuracy: 0.4785 - value_mse: 0.1890\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 32\n",
      "157/157 [==============================] - 16s 103ms/step - loss: 4.3691 - policy_loss: 1.8001 - value_loss: 0.5443 - policy_categorical_accuracy: 0.4821 - value_mse: 0.1902\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 33\n",
      "157/157 [==============================] - 16s 104ms/step - loss: 4.4427 - policy_loss: 1.8400 - value_loss: 0.5383 - policy_categorical_accuracy: 0.4712 - value_mse: 0.1873\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 34\n",
      "157/157 [==============================] - 16s 104ms/step - loss: 4.4436 - policy_loss: 1.8379 - value_loss: 0.5433 - policy_categorical_accuracy: 0.4685 - value_mse: 0.1895\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 35\n",
      "157/157 [==============================] - 16s 105ms/step - loss: 4.3804 - policy_loss: 1.8110 - value_loss: 0.5338 - policy_categorical_accuracy: 0.4834 - value_mse: 0.1856\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 36\n",
      "157/157 [==============================] - 16s 104ms/step - loss: 4.4205 - policy_loss: 1.8292 - value_loss: 0.5375 - policy_categorical_accuracy: 0.4719 - value_mse: 0.1872\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 37\n",
      "157/157 [==============================] - 17s 105ms/step - loss: 4.4185 - policy_loss: 1.8282 - value_loss: 0.5375 - policy_categorical_accuracy: 0.4773 - value_mse: 0.1871\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 38\n",
      "157/157 [==============================] - 16s 103ms/step - loss: 4.4307 - policy_loss: 1.8358 - value_loss: 0.5346 - policy_categorical_accuracy: 0.4711 - value_mse: 0.1862\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 39\n",
      "157/157 [==============================] - 16s 104ms/step - loss: 4.4154 - policy_loss: 1.8277 - value_loss: 0.5355 - policy_categorical_accuracy: 0.4777 - value_mse: 0.1865\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 40\n",
      "157/157 [==============================] - 16s 103ms/step - loss: 4.3388 - policy_loss: 1.7871 - value_loss: 0.5400 - policy_categorical_accuracy: 0.4822 - value_mse: 0.1880\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "val = [4.319472312927246, 1.780470371246338, 0.533981442451477, 0.4871000051498413, 0.18598230183124542]\n",
      "epoch 41\n",
      "157/157 [==============================] - 17s 105ms/step - loss: 4.4422 - policy_loss: 1.8367 - value_loss: 0.5443 - policy_categorical_accuracy: 0.4752 - value_mse: 0.1895\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 42\n",
      "157/157 [==============================] - 16s 104ms/step - loss: 4.4252 - policy_loss: 1.8302 - value_loss: 0.5401 - policy_categorical_accuracy: 0.4798 - value_mse: 0.1880\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 43\n",
      "157/157 [==============================] - 16s 104ms/step - loss: 4.3697 - policy_loss: 1.8027 - value_loss: 0.5397 - policy_categorical_accuracy: 0.4826 - value_mse: 0.1884\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 44\n",
      "157/157 [==============================] - 16s 103ms/step - loss: 4.3894 - policy_loss: 1.8111 - value_loss: 0.5426 - policy_categorical_accuracy: 0.4865 - value_mse: 0.1891\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 45\n",
      "157/157 [==============================] - 17s 106ms/step - loss: 4.3701 - policy_loss: 1.8033 - value_loss: 0.5389 - policy_categorical_accuracy: 0.4816 - value_mse: 0.1875\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 46\n",
      "157/157 [==============================] - 17s 105ms/step - loss: 4.3414 - policy_loss: 1.7886 - value_loss: 0.5395 - policy_categorical_accuracy: 0.4820 - value_mse: 0.1876\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 47\n",
      "157/157 [==============================] - 17s 106ms/step - loss: 4.4140 - policy_loss: 1.8259 - value_loss: 0.5376 - policy_categorical_accuracy: 0.4787 - value_mse: 0.1873\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 48\n",
      "157/157 [==============================] - 17s 106ms/step - loss: 4.3761 - policy_loss: 1.8055 - value_loss: 0.5406 - policy_categorical_accuracy: 0.4821 - value_mse: 0.1880\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 49\n",
      "157/157 [==============================] - 17s 107ms/step - loss: 4.3784 - policy_loss: 1.8083 - value_loss: 0.5372 - policy_categorical_accuracy: 0.4782 - value_mse: 0.1869\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 50\n",
      "157/157 [==============================] - 16s 105ms/step - loss: 4.3739 - policy_loss: 1.8053 - value_loss: 0.5388 - policy_categorical_accuracy: 0.4881 - value_mse: 0.1875\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "val = [4.313719749450684, 1.778023600578308, 0.5331237316131592, 0.4878000020980835, 0.18563765287399292]\n",
      "epoch 51\n",
      "157/157 [==============================] - 17s 107ms/step - loss: 4.3632 - policy_loss: 1.7984 - value_loss: 0.5418 - policy_categorical_accuracy: 0.4840 - value_mse: 0.1889\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 52\n",
      "157/157 [==============================] - 17s 109ms/step - loss: 4.3924 - policy_loss: 1.8128 - value_loss: 0.5423 - policy_categorical_accuracy: 0.4784 - value_mse: 0.1888\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 53\n",
      "157/157 [==============================] - 17s 106ms/step - loss: 4.4265 - policy_loss: 1.8333 - value_loss: 0.5354 - policy_categorical_accuracy: 0.4801 - value_mse: 0.1862\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 54\n",
      "157/157 [==============================] - 17s 106ms/step - loss: 4.3753 - policy_loss: 1.8052 - value_loss: 0.5405 - policy_categorical_accuracy: 0.4851 - value_mse: 0.1883\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 55\n",
      "157/157 [==============================] - 17s 105ms/step - loss: 4.3563 - policy_loss: 1.7969 - value_loss: 0.5380 - policy_categorical_accuracy: 0.4881 - value_mse: 0.1873\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 56\n",
      "157/157 [==============================] - 17s 105ms/step - loss: 4.3480 - policy_loss: 1.7926 - value_loss: 0.5382 - policy_categorical_accuracy: 0.4882 - value_mse: 0.1867\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 57\n",
      "157/157 [==============================] - 16s 105ms/step - loss: 4.3641 - policy_loss: 1.8006 - value_loss: 0.5383 - policy_categorical_accuracy: 0.4770 - value_mse: 0.1875\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 58\n",
      "157/157 [==============================] - 16s 105ms/step - loss: 4.3966 - policy_loss: 1.8164 - value_loss: 0.5393 - policy_categorical_accuracy: 0.4814 - value_mse: 0.1879\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 59\n",
      "157/157 [==============================] - 17s 105ms/step - loss: 4.3647 - policy_loss: 1.7967 - value_loss: 0.5468 - policy_categorical_accuracy: 0.4804 - value_mse: 0.1911\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 60\n",
      "157/157 [==============================] - 17s 105ms/step - loss: 4.4010 - policy_loss: 1.8149 - value_loss: 0.5467 - policy_categorical_accuracy: 0.4778 - value_mse: 0.1907\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "val = [4.308349609375, 1.7755402326583862, 0.5327222943305969, 0.4885999858379364, 0.18545590341091156]\n",
      "epoch 61\n",
      "157/157 [==============================] - 16s 103ms/step - loss: 4.4005 - policy_loss: 1.8201 - value_loss: 0.5357 - policy_categorical_accuracy: 0.4873 - value_mse: 0.1863\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 62\n",
      "157/157 [==============================] - 16s 104ms/step - loss: 4.4360 - policy_loss: 1.8359 - value_loss: 0.5397 - policy_categorical_accuracy: 0.4720 - value_mse: 0.1879\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 63\n",
      "157/157 [==============================] - 16s 104ms/step - loss: 4.3622 - policy_loss: 1.8032 - value_loss: 0.5311 - policy_categorical_accuracy: 0.4807 - value_mse: 0.1845\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 64\n",
      "157/157 [==============================] - 16s 103ms/step - loss: 4.3411 - policy_loss: 1.7896 - value_loss: 0.5373 - policy_categorical_accuracy: 0.4857 - value_mse: 0.1867\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.31523\n",
      "epoch 65\n",
      "157/157 [==============================] - 16s 103ms/step - loss: 4.3027 - policy_loss: 1.7707 - value_loss: 0.5368 - policy_categorical_accuracy: 0.4913 - value_mse: 0.1866\n",
      "\n",
      "Epoch 00001: loss improved from 4.31523 to 4.30271, saving model to checkpoint/cp.ckpt\n",
      "epoch 66\n",
      "157/157 [==============================] - 16s 104ms/step - loss: 4.4167 - policy_loss: 1.8265 - value_loss: 0.5391 - policy_categorical_accuracy: 0.4783 - value_mse: 0.1878\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 67\n",
      "157/157 [==============================] - 16s 104ms/step - loss: 4.3464 - policy_loss: 1.7926 - value_loss: 0.5366 - policy_categorical_accuracy: 0.4819 - value_mse: 0.1866\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 68\n",
      "157/157 [==============================] - 16s 104ms/step - loss: 4.3560 - policy_loss: 1.7964 - value_loss: 0.5387 - policy_categorical_accuracy: 0.4827 - value_mse: 0.1877\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 69\n",
      "157/157 [==============================] - 16s 104ms/step - loss: 4.3246 - policy_loss: 1.7802 - value_loss: 0.5396 - policy_categorical_accuracy: 0.4916 - value_mse: 0.1879\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 70\n",
      "157/157 [==============================] - 16s 104ms/step - loss: 4.3349 - policy_loss: 1.7872 - value_loss: 0.5359 - policy_categorical_accuracy: 0.4881 - value_mse: 0.1869\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "val = [4.302454948425293, 1.7728782892227173, 0.5321537256240845, 0.48980000615119934, 0.1852428913116455]\n",
      "epoch 71\n",
      "157/157 [==============================] - 17s 105ms/step - loss: 4.3820 - policy_loss: 1.8136 - value_loss: 0.5303 - policy_categorical_accuracy: 0.4783 - value_mse: 0.1844\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 72\n",
      "157/157 [==============================] - 16s 103ms/step - loss: 4.3602 - policy_loss: 1.7986 - value_loss: 0.5384 - policy_categorical_accuracy: 0.4824 - value_mse: 0.1878\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 73\n",
      "157/157 [==============================] - 17s 105ms/step - loss: 4.3685 - policy_loss: 1.8063 - value_loss: 0.5314 - policy_categorical_accuracy: 0.4816 - value_mse: 0.1852\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 74\n",
      "157/157 [==============================] - 16s 103ms/step - loss: 4.3612 - policy_loss: 1.7976 - value_loss: 0.5414 - policy_categorical_accuracy: 0.4842 - value_mse: 0.1886\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 75\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 4.3286 - policy_loss: 1.7838 - value_loss: 0.5364 - policy_categorical_accuracy: 0.4935 - value_mse: 0.1871\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 76\n",
      "157/157 [==============================] - 17s 106ms/step - loss: 4.3522 - policy_loss: 1.7977 - value_loss: 0.5323 - policy_categorical_accuracy: 0.4799 - value_mse: 0.1851\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 77\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 4.3220 - policy_loss: 1.7786 - value_loss: 0.5402 - policy_categorical_accuracy: 0.4798 - value_mse: 0.1884\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 78\n",
      "157/157 [==============================] - 16s 105ms/step - loss: 4.4018 - policy_loss: 1.8152 - value_loss: 0.5470 - policy_categorical_accuracy: 0.4802 - value_mse: 0.1904\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 79\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 4.3236 - policy_loss: 1.7829 - value_loss: 0.5332 - policy_categorical_accuracy: 0.4917 - value_mse: 0.1854\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 80\n",
      "157/157 [==============================] - 17s 105ms/step - loss: 4.3714 - policy_loss: 1.8087 - value_loss: 0.5294 - policy_categorical_accuracy: 0.4918 - value_mse: 0.1840\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "val = [4.299449920654297, 1.7714760303497314, 0.5319529175758362, 0.4896000027656555, 0.1851634979248047]\n",
      "epoch 81\n",
      "157/157 [==============================] - 16s 103ms/step - loss: 4.3600 - policy_loss: 1.7994 - value_loss: 0.5367 - policy_categorical_accuracy: 0.4824 - value_mse: 0.1865\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 82\n",
      "157/157 [==============================] - 16s 105ms/step - loss: 4.3252 - policy_loss: 1.7813 - value_loss: 0.5380 - policy_categorical_accuracy: 0.4869 - value_mse: 0.1872\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 83\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 4.4011 - policy_loss: 1.8184 - value_loss: 0.5397 - policy_categorical_accuracy: 0.4796 - value_mse: 0.1877\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 84\n",
      "157/157 [==============================] - 16s 104ms/step - loss: 4.3535 - policy_loss: 1.7943 - value_loss: 0.5402 - policy_categorical_accuracy: 0.4858 - value_mse: 0.1876\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 85\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 4.3226 - policy_loss: 1.7815 - value_loss: 0.5351 - policy_categorical_accuracy: 0.4882 - value_mse: 0.1863\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 86\n",
      "157/157 [==============================] - 16s 104ms/step - loss: 4.3475 - policy_loss: 1.7940 - value_loss: 0.5349 - policy_categorical_accuracy: 0.4816 - value_mse: 0.1857\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 87\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 4.3356 - policy_loss: 1.7863 - value_loss: 0.5384 - policy_categorical_accuracy: 0.4869 - value_mse: 0.1875\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 88\n",
      "157/157 [==============================] - 16s 105ms/step - loss: 4.3380 - policy_loss: 1.7890 - value_loss: 0.5355 - policy_categorical_accuracy: 0.4837 - value_mse: 0.1865\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 89\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 4.3405 - policy_loss: 1.7892 - value_loss: 0.5375 - policy_categorical_accuracy: 0.4853 - value_mse: 0.1873\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 90\n",
      "157/157 [==============================] - 16s 105ms/step - loss: 4.3360 - policy_loss: 1.7884 - value_loss: 0.5347 - policy_categorical_accuracy: 0.4824 - value_mse: 0.1860\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "val = [4.2952117919921875, 1.7697035074234009, 0.5312601327896118, 0.48910000920295715, 0.18487727642059326]\n",
      "epoch 91\n",
      "157/157 [==============================] - 16s 103ms/step - loss: 4.3519 - policy_loss: 1.7962 - value_loss: 0.5350 - policy_categorical_accuracy: 0.4830 - value_mse: 0.1862\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 92\n",
      "157/157 [==============================] - 17s 105ms/step - loss: 4.3903 - policy_loss: 1.8140 - value_loss: 0.5377 - policy_categorical_accuracy: 0.4814 - value_mse: 0.1872\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 93\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 4.3036 - policy_loss: 1.7726 - value_loss: 0.5340 - policy_categorical_accuracy: 0.4862 - value_mse: 0.1859\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 94\n",
      "157/157 [==============================] - 16s 104ms/step - loss: 4.3871 - policy_loss: 1.8092 - value_loss: 0.5441 - policy_categorical_accuracy: 0.4824 - value_mse: 0.1896\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 95\n",
      "157/157 [==============================] - 16s 103ms/step - loss: 4.3377 - policy_loss: 1.7888 - value_loss: 0.5356 - policy_categorical_accuracy: 0.4793 - value_mse: 0.1862\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 96\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 4.3421 - policy_loss: 1.7907 - value_loss: 0.5362 - policy_categorical_accuracy: 0.4829 - value_mse: 0.1867\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 97\n",
      "157/157 [==============================] - 17s 105ms/step - loss: 4.3520 - policy_loss: 1.7929 - value_loss: 0.5417 - policy_categorical_accuracy: 0.4838 - value_mse: 0.1888\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 98\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 4.4063 - policy_loss: 1.8181 - value_loss: 0.5455 - policy_categorical_accuracy: 0.4829 - value_mse: 0.1901\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 99\n",
      "157/157 [==============================] - 16s 105ms/step - loss: 4.3464 - policy_loss: 1.7930 - value_loss: 0.5358 - policy_categorical_accuracy: 0.4899 - value_mse: 0.1864\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 100\n",
      "157/157 [==============================] - 16s 103ms/step - loss: 4.3733 - policy_loss: 1.8024 - value_loss: 0.5440 - policy_categorical_accuracy: 0.4820 - value_mse: 0.1897\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "val = [4.291409015655518, 1.7679144144058228, 0.5310379266738892, 0.490200012922287, 0.18476761877536774]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101\n",
      "157/157 [==============================] - 17s 106ms/step - loss: 4.3501 - policy_loss: 1.7948 - value_loss: 0.5361 - policy_categorical_accuracy: 0.4813 - value_mse: 0.1868\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 102\n",
      "157/157 [==============================] - 17s 106ms/step - loss: 4.3513 - policy_loss: 1.7965 - value_loss: 0.5337 - policy_categorical_accuracy: 0.4815 - value_mse: 0.1856\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 103\n",
      "157/157 [==============================] - 17s 105ms/step - loss: 4.3744 - policy_loss: 1.8071 - value_loss: 0.5356 - policy_categorical_accuracy: 0.4756 - value_mse: 0.1866\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 104\n",
      "157/157 [==============================] - 16s 103ms/step - loss: 4.3957 - policy_loss: 1.8152 - value_loss: 0.5409 - policy_categorical_accuracy: 0.4839 - value_mse: 0.1882\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 105\n",
      "157/157 [==============================] - 16s 103ms/step - loss: 4.3550 - policy_loss: 1.7972 - value_loss: 0.5361 - policy_categorical_accuracy: 0.4830 - value_mse: 0.1867\n",
      "\n",
      "Epoch 00001: loss did not improve from 4.30271\n",
      "epoch 106\n",
      "145/157 [==========================>...] - ETA: 1s - loss: 4.3397 - policy_loss: 1.7899 - value_loss: 0.5353 - policy_categorical_accuracy: 0.4860 - value_mse: 0.1862"
     ]
    }
   ],
   "source": [
    "#!cp ./test-12052021_0829_last.h5 ./test-12052021_0829_previous.h5\n",
    "\n",
    "# 2nd and 3rd Train\n",
    "model.load_weights(\"./test-14052021_last.h5\")\n",
    "\n",
    "# learning rate schedule\n",
    "# def step_decay(epoch):\n",
    "# \tinitial_lrate = 0.000005 # 0.005\n",
    "# \tdrop = 0.1\n",
    "# \tepochs_drop = 2 # 10.0\n",
    "# \tlrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "# \treturn lrate\n",
    "\n",
    "# def lr_scheduler(epoch):\n",
    "#  if epoch < 100:\n",
    "#    return 0.00005\n",
    "#  elif epoch < 150:\n",
    "#    return 0.000025\n",
    "#  elif epoch >= 150:\n",
    "#    return 0.00001\n",
    "\n",
    "# checkpoint_path = \"training_1/cp.ckpt\"\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "#                                                 save_weights_only=True,\n",
    "#                                                 verbose=1)\n",
    "\n",
    "# adapt LR as epochs grows\n",
    "#lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# 1st model obtained using : \n",
    "# lr_callback = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
    "\n",
    "\n",
    "# checkpoint_path = '/checkpoint'\n",
    "checkpoint_path = \"checkpoint/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "\n",
    "callbacks_list = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='policy_categorical_accuracy', patience = 10, verbose = 1, mode = \"auto\",\n",
    "        restore_best_weights = True\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        save_weights_only=True,\n",
    "        monitor='loss',\n",
    "        mode='min',\n",
    "        save_best_only=True,\n",
    "        verbose =1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='loss',\n",
    "        # 1st Train\n",
    "        # factor = 0.2,\n",
    "        # patience = 10\n",
    "        # 2nd Train\n",
    "        # factor = 0.2,\n",
    "        # patience =  10       \n",
    "        # 3rdd Train\n",
    "        factor = 0.1,\n",
    "        patience = 3\n",
    "    ),\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir='/tf/my_log_dir',\n",
    "        histogram_freq=5,\n",
    "        embeddings_freq=0\n",
    "    )\n",
    "]\n",
    "\n",
    "for i in range (1, epochs + 1):\n",
    "    print ('epoch ' + str (i))\n",
    "    golois.getBatch (input_data, policy, value, end, groups)\n",
    "    history = model.fit(input_data,\n",
    "                        {'policy': policy, 'value': value}, \n",
    "                        epochs=1, \n",
    "                        callbacks=callbacks_list,\n",
    "                        batch_size=batch)\n",
    "    if (i % 10 == 0):\n",
    "        golois.getValidation (input_data, policy, value, end)\n",
    "        val = model.evaluate (input_data,\n",
    "                              [policy, value], verbose = 0, batch_size=batch)\n",
    "        print (\"val =\", val)\n",
    "    \n",
    "    if (i %100 == 0):\n",
    "            model.save ('/tf/my_log_dir/test-15052021_tmp.h5')\n",
    "\n",
    "# 1st Train\n",
    "# model.save ('test-14052021_last.h5')\n",
    "# 2nd Train\n",
    "model.save ('test-15052021_last_3rdTrain.h5')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "legrand-raussel-08052021.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
